{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60db2b0d-0f7b-4398-aad8-3f341873428c",
   "metadata": {},
   "source": [
    "# Prediction of Foraging States of *P. pacificus*\n",
    "\n",
    "This notebook will guide you through the prediction pipeline for foraging behaviours in *Pristionchus pacificus*.<br>\n",
    "You will already need to have data that was extracted by PharaGlow.<br>\n",
    "\n",
    "The single steps of this pipeline are the following:\n",
    "1. additional feature calculation\n",
    "2. model and augmentation loading\n",
    "3. data augmentation as defined by AugmentSelect file\n",
    "4. prediction\n",
    "5. visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59278ebe-f970-43fb-8910-3d49a147daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch, FancyArrowPatch\n",
    "import logging\n",
    "import yaml\n",
    "import json\n",
    "import joblib\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats.contingency import crosstab\n",
    "import networkx as nx\n",
    "from matplotlib.lines import Line2D\n",
    "import umap\n",
    "import itertools\n",
    "\n",
    "#home = os.path.expanduser(\"~\")\n",
    "sys.path.append(os.getcwd())\n",
    "from functions.load_model import load_tolist\n",
    "import functions.visualise as vis\n",
    "import functions.process as proc\n",
    "from functions.io import setup_logger, makedir\n",
    "from functions import FeatureEngine\n",
    "sys.path.append(os.path.expanduser('~'))\n",
    "from PpaPy.processing.preprocess import addhistory, select_features\n",
    "\n",
    "from numba import jit\n",
    "# set invalid (division by zero error) to ignore\n",
    "np.seterr(invalid='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f86000-ff2d-43d5-ba6c-69c551b22aa0",
   "metadata": {},
   "source": [
    "Please provide where your files are stored and where you would like your data to be saved in the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3416fe63-3b1e-45ee-bf2e-b6004746e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpath = f\"/gpfs/soma_fs/scratch/src/boeger/data_gueniz/\"\n",
    "#inpath = '/gpfs/soma_fs/gnb/gnb9201.bak/Mariannne Roca/MR_MS_pharaglow'\n",
    "inpath_with_subfolders = True\n",
    "inpath_pattern = ['Exp1_WT_larvae', 'Exp1_WT_OP50'] #exp1\n",
    "#inpath_pattern = ['Exp2_WT_larvae',\t'Exp2_tph1_larvae', 'Exp2_cat2_larvae',]# \n",
    "#inpath_pattern = ['Exp2_tbh1_larvae',  'Exp2_tdc1_larvae'] #exp2\n",
    "\n",
    "base_outpath = makedir('/gpfs/soma_fs/scratch/src/boeger/PpaPred_eren')\n",
    "#base_outpath = makedir('/gpfs/soma_fs/scratch/src/boeger/data_roca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098edf2b-a212-45ab-9dc2-231e536257d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = time.strftime(\"%Y%m%d\")\n",
    "datestr = time.strftime(\"%Y%m%d-%HH%MM\")\n",
    "home = os.path.expanduser(\"~\")\n",
    "#bac_data/ larvae_data/ self_data/  nothing_data/  tbh1_OP50/  tdc1_OP50/   tbh1_larvae/  tdc1_larvae/ nhr40_OP50/ tph1_larvae/ \n",
    "#cat2_OP50/ cat2_larvae/ tph1_larvae/ tph1_OP50/\n",
    "#ser3_larvae\n",
    "if inpath_with_subfolders:\n",
    "    new_inpath = [os.path.join(inpath, sub) for sub in os.listdir(inpath) if any(pat in sub for pat in inpath_pattern)]\n",
    "    inpath = new_inpath\n",
    "else:\n",
    "    inpath = [inpath]\n",
    "\n",
    "outpath, out_engine, out_predicted = [],[],[]\n",
    "for p in inpath:\n",
    "    in_folder = os.path.basename(p)\n",
    "    outpath.append(makedir(os.path.abspath(f\"{base_outpath}/{in_folder}\"))) # you can also use datestr to specify the outpath folder, like this makedir(os.path.abspath(f\"{datestr}_PpaPrediction\"))\n",
    "    out_engine.append(os.path.join(outpath[-1], in_folder+'_engine'))\n",
    "    #out_predicted.append(os.path.join(outpath[-1], in_folder+'_predicted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0755b3c1-ce83-4b47-9fa5-208939cd8c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.commonpath(inpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e41d02-d557-4bb2-9c16-0f6073423320",
   "metadata": {},
   "source": [
    "In the following section, standard model parameters are set. Change those only if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973dfd70-d9be-4677-a343-21492099560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(open(\"config.yml\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d359f1-66d9-4aab-b540-1848083856af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_color = config['cluster_color']\n",
    "cluster_group = config['cluster_group_man']\n",
    "cluster_label = config['cluster_names']\n",
    "clu_group_label = {_:f'{_}, {__}' for _, __ in tuple(zip([c for c in cluster_label.values()],[g for g in cluster_group.values()]))}\n",
    "skip_already = config['settings']['skip_already']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94241729-1769-48d5-81af-1e6119d504d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = config['settings']['model']\n",
    "version = os.path.basename(model_path).split(\"_\")[1].split(\".\")[0]\n",
    "ASpath = config['settings']['ASpath']\n",
    "smooth = config['settings']['fbfill']\n",
    "fps = config['settings']['fps']\n",
    "engine_done = []\n",
    "prediction_done = []\n",
    "\n",
    "logger_out = os.path.join(base_outpath,f\"{datestr}_PpaForagingPrediction.log\")\n",
    "logger = setup_logger('logger',filename=logger_out)\n",
    "logger.info(f\"Foraging prediction of Pristionchus pacificus\")\n",
    "logger.info(f\"Version of model == {version}, stored at {model_path}\\n\")\n",
    "log_inpath = '\\n'.join(inpath)\n",
    "logger.info(f\"Files to be predicted stored at:\\n{log_inpath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ede27ef-9a43-4d7d-a180-f506b3abbf68",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Feature Engineering\n",
    "In the following section, additional features are calculated.<br>\n",
    "The engineerd data files are saved under the specified outpath/subfolder.<br>\n",
    "(with subfolder being the inpath folder name postfixed by _engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c161a98a-656d-45fe-9177-c2f633f74d2e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "XYs, CLines  = FeatureEngine.run(inpath, out_engine, logger, return_XYCLine =True, skip_engine = False, skip_already=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ae0f08-53f4-433d-a670-9c029394d594",
   "metadata": {},
   "source": [
    "## 2. Load Model and Augmentation\n",
    "Here only the model- and augmentation-files are loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bcb83e-0028-4069-b577-2d202752b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model = joblib.load(open(model_path, 'rb'))\n",
    "augsel = joblib.load(ASpath)\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8120823-9947-43f6-9e38-e1eb3451294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "augsel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1135f087-122b-4f66-994f-f24897083fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95107d1-7111-44ef-bdfb-8d7306afa1fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_engine = [os.path.join(root, name) for root, dirs, files in os.walk(base_outpath) for name in files if 'engine' in os.path.basename(root) and any(pat in os.path.basename(root) for pat in inpath_pattern)]\n",
    "all_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46e8cfc-49c4-47de-9100-37624eaaf1dd",
   "metadata": {},
   "source": [
    "## 3. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7383b777-cf58-4fca-94b8-d52e8f03cef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_already = True\n",
    "for fpath in tqdm.tqdm(all_engine):\n",
    "    fn = os.path.basename(fpath)\n",
    "    dir_engine = os.path.dirname(fpath)\n",
    "    out_predicted = makedir(dir_engine[:-len('engine')]+'predicted')\n",
    "    out_fn = fn.replace('features', 'predicted')\n",
    "\n",
    "    if skip_already and out_fn in os.listdir(out_predicted):\n",
    "        continue\n",
    "    if not fn[0] == '.' and not out_fn in prediction_done and os.path.isfile(fpath):\n",
    "        d = load_tolist(fpath, droplabelcol=False)[0]\n",
    "        X = augsel.fit_transform(d)\n",
    "        col = X.columns\n",
    "        X = imp.fit_transform(X)\n",
    "        \n",
    "        pred = model.predict(X)\n",
    "        proba = model.predict_proba(X)\n",
    "        pred_smooth = proc.ffill_bfill(pred, smooth)\n",
    "        pred_smooth = np.nan_to_num(pred_smooth,-1)\n",
    "        proba_max = np.amax(proba, axis=1) ### New\n",
    "        proba_max_mean = pd.DataFrame(proba_max).rolling(30, min_periods=1).mean().values ### New\n",
    "        proba_low = np.all(proba_max_mean < .5, axis=1) ### New\n",
    "        pred_smooth[proba_low] = -1 ### NEW\n",
    "\n",
    "        #fn = os.path.basename(fn)\n",
    "        #out_fn = '_'.join(fn.split('_')[:4]+['predicted.json'])\n",
    "        p_out = pd.concat([d, pd.DataFrame(pred_smooth, columns=['prediction']), pd.DataFrame(model.predict_proba(X), columns=[f'proba_{i}' for i in range(proba.shape[1])])], axis=1)\n",
    "\n",
    "        jsnL = json.loads(p_out.to_json(orient=\"split\"))\n",
    "        jsnF = json.dumps(jsnL, indent = 4)\n",
    "        outpath_p = os.path.join(out_predicted,out_fn)\n",
    "        with open(outpath_p, \"w\") as outfile:\n",
    "            outfile.write(jsnF)\n",
    "\n",
    "        if 8 in pred_smooth:\n",
    "            print(f'WARNING! unexpected: {fn}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda366e2-2ce5-4df3-9df5-e4024d54b405",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2a54197-0b74-4a6f-acc3-3d84420fbf91",
   "metadata": {},
   "source": [
    "for fn in tqdm.tqdm(os.listdir(out_predicted)):\n",
    "    if fn[-13:] == 'predicted.csv' or fn[-14:] == 'predicted.json':\n",
    "        d = load_tolist(os.path.join(out_predicted,fn), droplabelcol=False)[0]\n",
    "        y_ps = d['prediction'].values\n",
    "\n",
    "        idx = pd.IndexSlice\n",
    "        onoff, dur, transi = proc.onoff_dict(y_ps, labels =np.unique(y_ps), return_duration=True, return_transitions=True)\n",
    "        data_describe = d.groupby(y_ps).describe().T.loc[idx[:, ['mean','std','count']], :].sort_index(level=0).T\n",
    "        dur_describe = pd.DataFrame(dur, columns=['duration']).groupby(transi).describe().T.loc[idx[:, ['mean','std','count']], :].sort_index(level=0).T\n",
    "        summary = pd.concat([dur_describe, data_describe], axis=1)\n",
    "        summary.index.name = 'cluster'\n",
    "        summary = summary.T.reset_index(drop=True).set_index(summary.T.index.map('_'.join)).T\n",
    "        summary = summary.set_index(summary.index.astype(int))\n",
    "        break\n",
    "        \n",
    "frame = d['prediction']\n",
    "frame = frame.rolling(30*2).apply(lambda s: s.mode()[0])\n",
    "\n",
    "trans_col,fr_transition = crosstab(frame[1:],frame[:-1], levels=([k for k in cluster_label if k != -1],[k for k in cluster_label if k != -1]))\n",
    "othersum_axis0 = fr_transition.sum(axis=0)-fr_transition.diagonal()\n",
    "transition_all = fr_transition/fr_transition.sum(axis=0)\n",
    "transition_toother = fr_transition/othersum_axis0\n",
    "transition_self = fr_transition.diagonal()/(fr_transition.sum(axis=0))\n",
    "np.fill_diagonal(transition_toother, 0)\n",
    "\n",
    "transition_merged = transition_all.copy()\n",
    "diag_idx = np.diag_indices(len(transition_merged))\n",
    "transition_merged[diag_idx] = transition_self\n",
    "transition_merged = pd.DataFrame(transition_all, columns = trans_col[1], index=trans_col[0]).fillna(0)\n",
    "\n",
    "cluster_alpha = ((summary['duration_mean']*summary['duration_count'])/len(y_ps)).reindex([k for k in cluster_label if k != -1]).fillna(0)\n",
    "\n",
    "#### TRANSITION PLOT\n",
    "transition_plot = vis.transition_plotter(transition_toother, cluster_color, transition_self, node_alpha=cluster_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246c535e-c0d5-4edd-a750-359b260e4612",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a59163-2655-4bac-9458-476287f4f40b",
   "metadata": {},
   "source": [
    "## 4. Prediction\n",
    "The augmented + predicted data files are saved under the specified outpath/subfolder.<br>\n",
    "(with subfolder being the inpath folder name postfixed by _predicted)<br>\n",
    "\n",
    "In the _predicted, plots of the bouts predicted over time along with the velocity and pumping rate are saved as pdf files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5dbd8-a16a-436b-bd59-d20a137d4d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_plotter(transition_toother, cluster_color, transition_self=None, figsize=(8,6), mut_scale=40, node_size=4000, \n",
    "                    other_connectionstyle = \"arc3,rad=.15\", self_connectionstyle=\"arc3,rad=0.5\", node_alpha = 1, exclude_label = [], clu_group_label=None):\n",
    "    if transition_self is None:\n",
    "        #print(transition_self)\n",
    "        transition_self = transition_toother.copy().diagonal()\n",
    "        np.fill_diagonal(transition_toother, 0)\n",
    "\n",
    "\n",
    "        \n",
    "    A = np.nan_to_num(np.around(transition_toother.T,3))\n",
    "    G = nx.from_numpy_matrix(A, create_using=nx.DiGraph)\n",
    "    \n",
    "    weights = nx.get_edge_attributes(G,'weight').values()\n",
    "    arr_out = [e[0] for e in G.edges(data=True)]\n",
    "                        \n",
    "    color_map = [cluster_color[k] for k in cluster_color if k not in exclude_label]\n",
    "    edge_color = [cluster_color[c-1] for c in arr_out]\n",
    "    #edge_alpha = [node_alpha[c] for c in arr_out]\n",
    "                        \n",
    "    fig, ax = plt.subplots(1, figsize=figsize)\n",
    "    fig_w = fig.get_size_inches()[0]\n",
    "    arrowsize = [w*mut_scale for w in weights]\n",
    "                        \n",
    "    if clu_group_label is None:\n",
    "        labels = dict(zip(range(len(G)),range(len(G))))\n",
    "    else:\n",
    "        labels = dict(zip(range(len(G)),  [clu_group_label[k] for k in clu_group_label if k not in exclude_label]))\n",
    "        \n",
    "    label_collection = nx.draw_networkx_labels(G, pos=nx.circular_layout(G), ax=ax, labels=labels)\n",
    "    \n",
    "    node_collection = nx.draw_networkx_nodes(G, pos=nx.circular_layout(G), ax=ax, node_color = color_map, node_size= node_size, margins=0.1,\n",
    "                                             alpha= node_alpha,\n",
    "                                             edgecolors=color_map)\n",
    "    edge_collection = nx.draw_networkx_edges(G, pos=nx.circular_layout(G), ax=ax, \n",
    "                                             arrowsize =arrowsize, connectionstyle=other_connectionstyle, arrowstyle=\"simple\",\n",
    "                                             label=list(weights), node_size=node_size, edge_color=edge_color)\n",
    "    \n",
    "    ### to self\n",
    "    edgelist = [i for i in G.nodes() if transition_self[i] > 0 and transition_self[i] != np.nan]\n",
    "    selfweights = {i:transition_self[i] for i in edgelist}\n",
    "    G.add_edges_from([(i,i) for i in edgelist])\n",
    "    \n",
    "    for i in edgelist:\n",
    "        cor = np.round(nx.circular_layout(G)[i],2)\n",
    "        rad = np.arctan2(*cor)-np.arctan2(0,0)\n",
    "        rad_s, rad_t = rad-.15, rad+.15\n",
    "        vl = np.linalg.norm(cor)+.2\n",
    "        xy_t = [vl*np.sin(rad_s),vl*np.cos(rad_s)]\n",
    "        xy_s = [vl*np.sin(rad_t),vl*np.cos(rad_t)]\n",
    "        (A, _, C, D) =  vis.SemiCirc_coordinates(xy_s, xy_t, r=0.2)\n",
    "        arrow0 = FancyArrowPatch(posA=A, posB=D, connectionstyle=self_connectionstyle, arrowstyle=\"simple\", mutation_scale= selfweights[i]*mut_scale, color=color_map[i])\n",
    "        arrow1 = FancyArrowPatch(posA=D, posB=C, connectionstyle=self_connectionstyle, arrowstyle=\"simple\", mutation_scale= selfweights[i]*mut_scale, color=color_map[i])\n",
    "        ax.add_artist(arrow0)\n",
    "        ax.add_artist(arrow1)\n",
    "    \n",
    "        \n",
    "    for arr_s in np.linspace(0.2,1,5):\n",
    "        arrow = FancyArrowPatch((1.6, arr_s), (1.9, arr_s), mutation_scale=arr_s*mut_scale, label = arr_s, color='k', alpha=0.5)\n",
    "        ax.text(1.95, arr_s-0.03, f\"{int(arr_s*100)}%\")\n",
    "        ax.add_patch(arrow)\n",
    "    ax.set_xlim(-2,2)\n",
    "    ax.set_ylim(-1.5,1.5)\n",
    "    ax.axis('off')\n",
    "    return fig\n",
    "\n",
    "def ethogram_plotter(d, y, onoff,  smooth, cluster_color, figsize=(20,5), fps=30,xtick_spread=30, d_toplot=['velocity', 'rate'], d_bar_alpha =0.3):\n",
    "    timeinsec = np.array(range(len(d)))/fps\n",
    "    \n",
    "    fig, axs = plt.subplots(3,1, figsize=figsize,constrained_layout=True)\n",
    "    \n",
    "    for c in np.unique(y).astype(int):\n",
    "        axs[0].broken_barh(onoff[c],(0,1),facecolors = cluster_color[c])\n",
    "    axs[0].set_xticks(range(len(timeinsec))[::xtick_spread*fps])\n",
    "    axs[0].set_xticklabels(timeinsec[::xtick_spread*fps].astype(int))\n",
    "    axs[0].set_title(f'Cluster preditcion (smoothed {smooth/fps} sec).')\n",
    "    axs[0].xaxis.set_minor_locator(plt.MultipleLocator(5*fps))\n",
    "    for i,c in enumerate(d_toplot):\n",
    "        for c_ in np.unique(y).astype(int):\n",
    "            #axs[i+1].broken_barh(onoff[c_],(min(d[c]),max(d[c])),facecolors = cluster_color[c_], alpha=0.6, zorder=0)\n",
    "            axs[i+1].broken_barh(onoff[c_],(np.nanmin(d[c]),np.nanmax(d[c])-np.nanmin(d[c])),facecolors = cluster_color[c_], alpha=d_bar_alpha, zorder=0)\n",
    "        axs[i+1].plot(d[c].rolling(30, min_periods=0).mean(),c='k')\n",
    "        axs[i+1].set_xticks(range(len(timeinsec))[::xtick_spread*fps])\n",
    "        axs[i+1].set_xticklabels(timeinsec[::xtick_spread*fps].astype(int))\n",
    "        axs[i+1].set_title(f\"{c} (smoothed, 1 sec)\")\n",
    "        axs[i+1].xaxis.set_minor_locator(plt.MultipleLocator(5*fps))\n",
    "    axs[2].set_xlabel('sec')\n",
    "    \n",
    "    plt.legend(handles=[Patch(facecolor=cluster_color[i]) for i in np.unique(y).astype(int)],\n",
    "          labels=[clu_group_label[k] for k in cluster_label if k in np.unique(y)],\n",
    "          ncol=3, loc='upper left',\n",
    "          bbox_to_anchor=(0, -0.5))\n",
    "    fig.suptitle(f'Ethogram of {fn}',fontsize=16)\n",
    "    return fig\n",
    "    \n",
    "def CLtrajectory_plotter(CLine, XY, y, cluster_color, cluster_label, figsize=(10,10)):\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    legend_elements = [Line2D([0], [0],color=cluster_color[i], label=cluster_label [i]) for i in cluster_label]\n",
    "    adjustCL = (CLine-np.nanmean(CLine))+np.repeat(XY.reshape(XY.shape[0],1,XY.shape[1]), CLine.shape[1], axis=1)-np.nanmean(XY, axis=0)# fits better than subtracting 50\n",
    "    adjustXY = XY-np.nanmean(XY, axis=0)\n",
    "    for l in np.unique(y).astype(int):\n",
    "    #for l in [2,3,5,8]:#[1,2,6,7]#[2,3,5,8]\n",
    "        #if l != 6:\n",
    "        il = np.where(y == l)[0]\n",
    "        ax.plot(*adjustCL[il].T, c=cluster_color[l], alpha = 0.1)#cluster_color[l]\n",
    "            #plt.scatter(XY[:,0][il],XY[:,1][il], marker=\".\", lw=2, c=bar_c[l], alpha=0.1)\n",
    "    ax.set_title(fn)\n",
    "    ax.axis('equal')\n",
    "    ax.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(1,1))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab907153-8032-4229-ab36-86808056b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpIntEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a026588-eaef-4b62-b747-d3d29a6580cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ethograms = True\n",
    "summaries = True\n",
    "transitions = True\n",
    "trajectories = True\n",
    "\n",
    "all_predicted = [os.path.join(root, name) for root, dirs, files in os.walk(base_outpath) for name in files if 'predicted' in os.path.basename(root) and any(pat in os.path.basename(root) for pat in inpath_pattern) and 'predicted.json' in name]\n",
    "#for fn in tqdm.tqdm(os.listdir(out_predicted)):\n",
    "#    if fn[-13:] == 'predicted.csv' or fn[-14:] == 'predicted.json':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb1c47-28e5-4eaf-94ee-ef4aee5deaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fbc8a0-3414-4ee1-b8d9-16a00411158a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for fpath in tqdm.tqdm(all_predicted):\n",
    "    \n",
    "    fn = os.path.basename(fpath)\n",
    "    fn_out = fn.replace('predicted.json','')\n",
    "    out_predicted = os.path.dirname(fpath)\n",
    "    \n",
    "    d = load_tolist(os.path.join(out_predicted,fn), droplabelcol=False)[0]\n",
    "    y_ps = d['prediction'].values\n",
    "    d['prediction'].to_csv(os.path.join(out_predicted, fn_out+'prediction.csv'), index=False)\n",
    "        \n",
    "    if ethograms:            \n",
    "        onoff = proc.onoff_dict(y_ps, labels =np.unique(y_ps))\n",
    "        onoff = {int(k):v for k,v in onoff.items()}\n",
    "        with open(os.path.join(out_predicted, fn_out+'_onoff.json'), \"w\") as onoff_out: \n",
    "            json.dump(onoff,onoff_out,cls=NpIntEncoder)\n",
    "        ethogram_plot = ethogram_plotter(d, y_ps, onoff,  smooth, cluster_color)\n",
    "        #plt.savefig('clusterbouts.pdf')\n",
    "        plt.savefig(os.path.join(out_predicted, fn_out+'_predictedbouts.pdf'))\n",
    "        plt.show()\n",
    "\n",
    "    if summaries:\n",
    "        idx = pd.IndexSlice\n",
    "        onoff, dur, transi = proc.onoff_dict(y_ps, labels =np.unique(y_ps), return_duration=True, return_transitions=True)\n",
    "        data_describe = d.groupby(y_ps).describe().T.loc[idx[:, ['mean','std','count']], :].sort_index(level=0).T\n",
    "        dur_describe = pd.DataFrame(dur, columns=['duration']).groupby(transi).describe().T.loc[idx[:, ['mean','std','count']], :].sort_index(level=0).T\n",
    "        dur_describe['duration','relative'] = pd.DataFrame(dur, columns=['duration']).groupby(transi).apply(lambda cd: cd.sum()/len(d))\n",
    "        summary = pd.concat([dur_describe, data_describe], axis=1)\n",
    "        summary.index.name = 'cluster'\n",
    "        summary = summary.T.reset_index(drop=True).set_index(summary.T.index.map('_'.join)).T\n",
    "        summary = summary.set_index(summary.index.astype(int))\n",
    "        summary = summary.reindex([k for k in cluster_label if k != -1])\n",
    "        summary.to_csv(os.path.join(out_predicted, fn_out+'summary.csv'))\n",
    "    \n",
    "    if transitions:\n",
    "        y_ps_transition = pd.DataFrame(y_ps).rolling(30).apply(lambda s: s.mode()[0])[29::30].values.flatten()\n",
    "        \n",
    "        trans_col,fr_transition = crosstab(y_ps_transition[1:],y_ps_transition[:-1],\n",
    "                                           levels=([k for k in cluster_label],[k for k in cluster_label])\n",
    "                                          )\n",
    "        #othersum_axis0 = fr_transition.sum(axis=0)-fr_transition.diagonal()\n",
    "        transition_all = fr_transition/fr_transition.sum(axis=0)\n",
    "        #transition_toother = fr_transition/othersum_axis0\n",
    "        #transition_self = fr_transition.diagonal()/(fr_transition.sum(axis=0))\n",
    "        #np.fill_diagonal(transition_toother, 0)\n",
    "        \n",
    "        #transition_merged = transition_toother.copy()\n",
    "        #diag_idx = np.diag_indices(len(transition_merged))\n",
    "        #transition_merged[diag_idx] = transition_self\n",
    "        transition_merged = pd.DataFrame(transition_all, columns = trans_col[1], index=trans_col[0])#.fillna(0) #should not fill nan with 0!\n",
    "        transition_merged.to_csv(os.path.join(out_predicted, fn_out+'transitions.csv'))\n",
    "        \n",
    "        #### TRANSITION PLOT\n",
    "    \n",
    "        #transition_plot = transition_plotter(transition_all, cluster_color, node_alpha=summary['duration_relative'].fillna(0).tolist())\n",
    "        #plt.savefig(os.path.join(out_predicted,fn_out+'clustertransitions.pdf'))\n",
    "    \n",
    "\n",
    "    if trajectories:\n",
    "        XY = XYs[fn.replace('_predicted.json','.json_labeldata.csv')]\n",
    "        CLine = CLines[fn.replace('_predicted.json','.json_labeldata.csv')]\n",
    "\n",
    "\n",
    "        \n",
    "        CLtrajectory_plot = CLtrajectory_plotter(CLine, XY, y_ps, cluster_color, cluster_label, figsize=(10,10),)\n",
    "        plt.savefig(os.path.join(out_predicted, fn_out+'CLtrajectory.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad94e24-0a5c-4a52-ad9d-bb87cab0a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dur_describe = pd.DataFrame(dur, columns=['duration']).groupby(transi).describe().T.loc[idx[:, ['mean','std','count']], :].sort_index(level=0).T\n",
    "dur_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be15b19-93e2-4e09-97f1-a52ac2f04518",
   "metadata": {},
   "outputs": [],
   "source": [
    "dur_describe['duration','relative'] = pd.DataFrame(dur, columns=['duration']).groupby(transi).apply(lambda cd: cd.sum()/len(d))\n",
    "dur_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2998c04-51dd-4f57-88bb-3d126eba508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(dur_describe['duration','relative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4987834-9e91-44b6-95a1-d79400d9e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "onoff = {1: [(0, 99), (200,99)],\n",
    "         2: [(100,99)]}\n",
    "\n",
    "with open(\"sample.json\", \"w\") as outfile: \n",
    "    json.dump(onoff, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccbb932-411a-4965-b199-2c0f0b73f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('sample.json',) \n",
    "data = json.load(f)\n",
    "f.close()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4a9c3b-989d-4ada-b932-503e64465c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "color = {1:'blue',2:'red'}\n",
    "for c in [1,2]:\n",
    "    plt.broken_barh(onoff[c],(0,1),facecolors = color[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c462a9d-520a-42d2-8f1a-d9dc7427e83a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loc = {}\n",
    "for fn in os.listdir(out_predicted):\n",
    "    if \"predicted.json\" in fn:\n",
    "        loc[fn]= os.path.join(out_predicted, fn)\n",
    "\n",
    "data_batch = load_tolist(loc, droplabelcol=False)\n",
    "data_batch_concat = pd.concat([d for d in data_batch], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd825ea-c4ce-4ef5-8313-d390a431b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_batch_concat = data_batch_concat['prediction']\n",
    "y_batch = [d['prediction'] for d in data_batch]\n",
    "fn = out_predicted+'_batch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1a4d44-38f6-4eec-80b6-ef336824a187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onoff_dict(arr_raw, labels = range(-1,4), return_duration=False, return_transitions = False, return_all = False, treatasone=True):\n",
    "\n",
    "    if not isinstance(arr_raw, list):\n",
    "        arr_raw = [arr_raw]\n",
    "\n",
    "    arr_onoff = {}\n",
    "    arr_transi =  []\n",
    "    arr_onset =  []\n",
    "    arr_onnext =  []\n",
    "    arr_dur =  []\n",
    "    total_dur = 0\n",
    "    for i,a in enumerate(arr_raw):\n",
    "        if isinstance(a, pd.Series) or isinstance(a, pd.DataFrame):\n",
    "            a = a.values\n",
    "        arr_s = a[1:]\n",
    "        arr = a[:-1]\n",
    "\n",
    "        transi = np.append(arr[arr != arr_s], arr[-1])\n",
    "        onset = (np.concatenate([[0],np.where([arr != arr_s])[1]+1]))\n",
    "        onnext = (np.append(np.array((onset)[1:]), [len(arr)+1]))\n",
    "        dur = (onnext)-onset\n",
    "        arr_transi.append(transi)\n",
    "        arr_onset.append(onset)\n",
    "        arr_onnext.append(onnext)\n",
    "        arr_dur.append(dur)\n",
    "\n",
    "        if treatasone:\n",
    "            if i > 0:\n",
    "                total_dur += arr_onnext[i-1][-1]\n",
    "\n",
    "        for b in np.unique(a):\n",
    "            b_idx = np.where(transi == b)\n",
    "            b_onoff = list(zip(onset[b_idx]+total_dur, dur[b_idx]+total_dur))\n",
    "            if b in arr_onoff.keys():\n",
    "                arr_onoff[b] = arr_onoff[b]+b_onoff\n",
    "            else:\n",
    "                arr_onoff[b] = b_onoff\n",
    "    if treatasone:\n",
    "        arr_dur = np.concatenate(arr_dur)\n",
    "        arr_transi = np.concatenate(arr_transi)\n",
    "        arr_onset = np.concatenate([a+arr_onnext[i-1][-1] if i > 0 else a for i,a in enumerate(arr_onset)])\n",
    "        arr_onnext = np.concatenate([a+arr_onnext[i-1][-1] if i > 0 else a for i,a in enumerate(arr_onnext)])\n",
    "        # might hav to work here on further, change how arr_onset and arr_onnext are daved\n",
    "\n",
    "    if return_all == True:\n",
    "        return arr_onoff, arr_dur, arr_transi, arr_onset, arr_onnext\n",
    "    elif return_transitions == True and return_duration == True:\n",
    "        return arr_onoff, arr_dur, arr_transi\n",
    "    elif return_duration == True or return_transitions == True:\n",
    "        if return_duration == True:\n",
    "            return arr_onoff, arr_dur\n",
    "        if return_transitions == True:\n",
    "            return arr_onoff, arr_transi\n",
    "    else:\n",
    "        return arr_onoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdf2dfa-77eb-4c77-9208-97f9595c1bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "onoff, dur, transi = onoff_dict(y_batch, labels =np.unique(y_batch_concat), return_duration=True, return_transitions=True)\n",
    "data_describe = data_batch_concat.groupby(y_batch_concat).describe().T.loc[idx[:, ['mean','std','count']], :].sort_index(level=0).T\n",
    "dur_describe = pd.DataFrame(dur, columns=['duration']).groupby(transi).describe().T.loc[idx[:, ['mean','std','count']], :].sort_index(level=0).T\n",
    "dur_describe['duration','relative'] = pd.DataFrame(dur, columns=['duration']).groupby(transi).apply(lambda cd: cd.sum()/len(y_batch_concat))\n",
    "summary = pd.concat([dur_describe, data_describe], axis=1)\n",
    "summary.index.name = 'cluster'\n",
    "summary = summary.T.reset_index(drop=True).set_index(summary.T.index.map('_'.join)).T\n",
    "summary = summary.set_index(summary.index.astype(int))\n",
    "summary.to_csv(os.path.join(out_predicted, os.path.basename(outpath)+'_batch_summary.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eeb050-1edb-4731-b2d4-7e94ea6cfc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "transi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d9a517-07ee-4f65-a0c3-b36bef9a7e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,d in enumerate(data_batch):\n",
    "    frame = d['prediction'].rolling(30).apply(lambda s: s.mode()[0])[29::30].values.flatten()\n",
    "    trans_col_,fr_transition_ = crosstab(frame[1:], frame[:-1], levels=([k for k in cluster_label if k != -1],[k for k in cluster_label if k != -1]))\n",
    "    fr_transition_ = pd.read_csv(d) ##################################read transitions.csv should look like normal fr_transition, get trans_col from header\n",
    "    if i == 0:\n",
    "        fr_transition = fr_transition_\n",
    "        trans_col = trans_col_\n",
    "    if trans_col_ == trans_col:\n",
    "        fr_transition += fr_transition_\n",
    "    else:\n",
    "        print('WARNING')\n",
    "    #fr_transition/fr_transition.sum(axis=0)\n",
    "\n",
    "#othersum_axis0 = fr_transition.sum(axis=0)-fr_transition.diagonal()\n",
    "#transition_toother = fr_transition/othersum_axis0\n",
    "#transition_self = fr_transition.diagonal()/(othersum_axis0+fr_transition.diagonal())\n",
    "#np.fill_diagonal(transition_toother, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6011c50-0588-46b3-9248-4abf98856c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_all = fr_transition/fr_transition.sum(axis=0)\n",
    "#diag_idx = np.diag_indices(len(transition_toother))\n",
    "#transition_all[diag_idx] = transition_self\n",
    "#transition_all = pd.DataFrame(transition_all, columns = trans_col[1], index=trans_col[0])\n",
    "transition_csv = pd.DataFrame(transition_all, columns = trans_col[1], index=trans_col[0]).fillna(0)\n",
    "transition_csv.to_csv(os.path.join(out_predicted, os.path.basename(outpath)+'_batch_transitions.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bca502-77e0-4284-ba85-b9b8d8fd022d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transition_plot = transition_plotter(transition_all, cluster_color, node_alpha=dur_describe['duration','relative'].tolist())\n",
    "plt.text(1.5, -1, f'{in_folder}\\nN = {len(data_batch)}', fontsize=12)\n",
    "plt.savefig(os.path.join(out_predicted, os.path.basename(outpath)+'_batch_transitions.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ee27e-443b-4bb4-837d-dff3300b883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fr_transition/fr_transition.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db564a69-ac51-4b38-bc44-d543fee006fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MarkovChain(object):\n",
    "    def __init__(self, transition_matrix, states):\n",
    "        \"\"\"\n",
    "        Initialize the MarkovChain instance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        transition_matrix: 2-D array\n",
    "            A 2-D array representing the probabilities of change of \n",
    "            state in the Markov Chain.\n",
    "\n",
    "        states: 1-D array \n",
    "            An array representing the states of the Markov Chain. It\n",
    "            needs to be in the same order as transition_matrix.\n",
    "        \"\"\"\n",
    "        self.transition_matrix = np.atleast_2d(transition_matrix)\n",
    "        self.states = states\n",
    "        self.index_dict = {self.states[index]: index for index in \n",
    "                           range(len(self.states))}\n",
    "        self.state_dict = {index: self.states[index] for index in\n",
    "                           range(len(self.states))}\n",
    "\n",
    "    def next_state(self, current_state):\n",
    "        \"\"\"\n",
    "        Returns the state of the random variable at the next time \n",
    "        instance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        current_state: str\n",
    "            The current state of the system.\n",
    "        \"\"\"\n",
    "        return np.random.choice(\n",
    "         self.states, \n",
    "         p=self.transition_matrix[self.index_dict[current_state], :]\n",
    "        )\n",
    "\n",
    "    def generate_states(self, current_state, no=10):\n",
    "        \"\"\"\n",
    "        Generates the next states of the system.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        current_state: str\n",
    "            The state of the current random variable.\n",
    "\n",
    "        no: int\n",
    "            The number of future states to generate.\n",
    "        \"\"\"\n",
    "        future_states = []\n",
    "        for i in range(no):\n",
    "            next_state = self.next_state(current_state)\n",
    "            future_states.append(next_state)\n",
    "            current_state = next_state\n",
    "        return np.array(future_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4c9f0-7cf4-4370-83a7-62ab6a424815",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix = np.nan_to_num(fr_transition/fr_transition.sum(axis=0))\n",
    "np.round(transition_matrix,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e9c6f-e840-4879-8981-34dd7d691448",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_chain = MarkovChain(transition_matrix=transition_matrix.T, \n",
    "                           states=[0,1,2,3,4,5])\n",
    "\n",
    "predicted = (markov_chain.generate_states(current_state=4, no=200))\n",
    "plt.plot(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e2adf-353b-4ba5-b50d-595e224a7eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ethogram_only_plotter(y, onoff, cluster_color, fn, figsize=(20,2), fps=30,xtick_spread=100,):\n",
    "    timeinsec = np.array(range(len(y)))/fps\n",
    "    \n",
    "    fig, ax = plt.subplots(1, figsize=figsize,constrained_layout=True)\n",
    "    \n",
    "    for c in np.unique(y).astype(int):\n",
    "        ax.broken_barh(onoff[c],(0,1),facecolors = cluster_color[c])\n",
    "    ax.set_xticks(range(len(timeinsec))[::xtick_spread*fps])\n",
    "    ax.set_xticklabels(timeinsec[::xtick_spread*fps].astype(int))\n",
    "    ax.set_title(f'Cluster preditcion')\n",
    "    ax.set_xlabel('sec')\n",
    "\n",
    "    plt.legend(handles=[Patch(facecolor=cluster_color[i]) for i in np.unique(y).astype(int)],\n",
    "          labels=[clu_group_label[k] for k in cluster_label if k in np.unique(y)],\n",
    "          ncol=3, loc='upper left',\n",
    "          bbox_to_anchor=(0, -0.5))\n",
    "    fig.suptitle(f'Ethogram of {fn}',fontsize=16)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec6193-22bb-4603-b5f6-e861f7c847f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "onoff_predicted = proc.onoff_dict(predicted, labels =np.unique(y_ps))\n",
    "ethogram_predicted = ethogram_only_plotter(predicted, onoff_predicted, cluster_color, fn = 'nhr40 predicted', fps =1, xtick_spread=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00121468-8808-452c-84f3-f1aac3e6c8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env2",
   "language": "python",
   "name": "sklearn-env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
