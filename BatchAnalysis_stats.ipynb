{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b5538c-c780-4848-9d27-e2079b7f5f5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch, FancyArrowPatch\n",
    "from matplotlib import colors\n",
    "import logging\n",
    "import yaml\n",
    "import json\n",
    "import joblib\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats.contingency import crosstab\n",
    "import networkx as nx\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib as mpl\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "from functions.load_model import load_tolist\n",
    "import functions.visualise as vis\n",
    "import functions.process as proc\n",
    "from functions.io import setup_logger, makedir\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58639dd-1006-434c-b377-9f58158a83fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_keys(json_file, key):\n",
    "    def traverse_dict(d, key):\n",
    "        if isinstance(d, dict):\n",
    "            if key in d.keys():\n",
    "                return {key: d[key]}\n",
    "            else:\n",
    "                return {k: traverse_dict(v, key) for k, v in d.items()}\n",
    "        elif isinstance(d, list):\n",
    "            return [traverse_dict(x, key) for x in d]\n",
    "        else:\n",
    "            return d\n",
    "\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    nested_dict = traverse_dict(data, key)\n",
    "    return {(innerKey, outerKey): values for outerKey, innerDict in nested_dict.items() for innerKey, values in innerDict.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a1a5f-0c8b-4243-a76a-c5ab0059b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def sig_stars(p):\n",
    "    if p <= .01:\n",
    "        return min(int(-np.ceil(np.log10(abs(p)))),4) # should be correct needs checking\n",
    "    elif p <= .05:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def find_common(*lists):\n",
    "    common = [value for value in lists[0] if all(value in lst for lst in lists)]\n",
    "    return common\n",
    "\n",
    "def find_uncommon(*lists):\n",
    "    all_items = [item for lst in lists for item in lst]\n",
    "    common = find_common(*lists)\n",
    "    uncommon = [item for item in all_items if item not in common]\n",
    "    return uncommon\n",
    "\n",
    "def find_uncommon(*lists):\n",
    "    all_items = np.array(lists)\n",
    "    df_items = pd.DataFrame(lists)\n",
    "    common = find_common(*all_items)\n",
    "    common_bool = np.isin(df_items,common)\n",
    "    uncommon = []\n",
    "    for i,col in enumerate(common_bool):\n",
    "        col_uncommon = df_items.loc[i,~col]\n",
    "        uncommon.append('_'.join(col_uncommon.dropna()))\n",
    "    return uncommon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18efb6f1-ab9d-4f9b-a4ec-50a1e27fd49c",
   "metadata": {},
   "source": [
    "Please provide where your files are stored and where you would like your data to be saved in the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a3bd5-29bf-44e3-bbe5-231912f5022c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datestr = time.strftime(\"%Y%m%d-%Hh%Mm\")\n",
    "inpath = \"/gpfs/soma_fs/home/boeger/PpaPred/ErenBoeger_2024/\" \n",
    "#inpath = '/gpfs/soma_fs/home/boeger/PpaPred/PpaPred_roca_35727184'\n",
    "\n",
    "config_path = \"config.yml\"\n",
    "\n",
    "# Eren Boeger 2024\n",
    "include, fig_folder = [\"Exp1_WT_larvae\", \"Exp1_WT_OP50\"], \"Exp1\"   \n",
    "#include, fig_folder = [\"Exp2_WT_larvae\", \"Exp2_tph1_larvae\", \"Exp2_cat2_larvae\", \"Exp2_tdc1_larvae\", \"Exp2_tbh1_larvae\"], \"Exp2\"\n",
    "include, fig_folder = [\"Exp3_tbh1_larvae\", \"Exp3_tdc1_larvae\", \"Exp3_tyra2_larvae\", \"Exp3_ser2_larvae\", \"Exp3_tyra3_larvae\", \"Exp3_lgc55_larvae\"], \"Exp3b\"\n",
    "include, fig_folder = [\"Exp3_WT_larvae\", \"Exp3_tbh1_larvae\", \"Exp3_ser3_larvae\", \"Exp3_ser6_larvae\", \"Exp3_octr1_larvae\"], \"Exp3a\"\n",
    "include, fig_folder = [\"Supp5_WT_larvae\", \"Supp5_tyramine_larvae\", \"Supp5_octopamine_larvae\"], \"Supp5\"\n",
    "include, fig_folder = [\"Exp2_WT_larvae\", \"Supp4_tbh1tdc1_larvae\"], \"Supp4\"\n",
    "include, fig_folder = [\"Exp3_WT_larvae\", \"Supp7_ser2tyra2tyra3_larvae\"], \"Supp7\"\n",
    "\n",
    "#include, fig_folder = ['L147', 'L157', 'L176', 'L118', 'L156'], 'fig_folder' #['L118','L147','L156','L157','L176']\n",
    "#include, fig_folder  = [\"L147\", \"L157\", \"L176\", \"L118\", \"L119\", \"L156\"], 'Fig'\n",
    "\n",
    "outpath = makedir(os.path.join(inpath,fig_folder))\n",
    "file_pattern = 'batch.json'\n",
    "stat_pop = f'{include[0]}_batch.json'\n",
    "inpath_with_subfolders = True\n",
    "use_bonferroni = True\n",
    "\n",
    "WT_ordering = False #[1., 0., 2., 6., 8., 3., 4., 7., 5.]\n",
    "plot = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfb2869-bbe9-49fa-bee4-a11a39a8e5f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#all_transitions = None\n",
    "#all_durations = None\n",
    "\n",
    "### I/O ################################################\n",
    "all_files = [os.path.join(root, name) for root, dirs, files in os.walk(inpath) for name in files if file_pattern in name and any([c in name for c in include])]\n",
    "all_files.sort(key = lambda i: np.where([c in i for c in include])) # to sort all_files as given in include\n",
    "loc_all = {os.path.basename(f):f for f in all_files}\n",
    "\n",
    "if outpath is None:\n",
    "    outpath = os.path.commonpath(all_files)\n",
    "    \n",
    "### Configuration ################################################\n",
    "config = yaml.safe_load(open(config_path, \"r\"))\n",
    "cluster_color = config['cluster_color']\n",
    "cluster_group = config['cluster_group_man']\n",
    "cluster_label = config['cluster_names']\n",
    "clu_group_label = {_:f'{_}, {__}' for _, __ in tuple(zip([c for c in cluster_label.values()],[g for g in cluster_group.values()]))}\n",
    "skip_already = config['settings']['skip_already']\n",
    "fps = 30\n",
    "pctl_toplot = (0,100)\n",
    "showfliers=False\n",
    "loc_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56067c8-5221-402c-99b5-94b6c3d8624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "test_metrics = ['mean duration', 'rel time in']#'mean transitions' #'mean duration',\n",
    "for metric in test_metrics:\n",
    "    outmetric = os.path.join(outpath, f\"{datestr}_{''.join(metric.split(' '))}_Utest\")\n",
    "    \n",
    "    metric_multi = None\n",
    "    for fn, fpath in loc_all.items():\n",
    "        batch_metric = pd.DataFrame(load_data_from_keys(fpath, metric)).droplevel(0, axis=1)\n",
    "        batch_metric.columns = pd.MultiIndex.from_product([[fn], batch_metric.columns])\n",
    "        if metric_multi is None:\n",
    "            metric_multi = batch_metric\n",
    "        else:\n",
    "            metric_multi = pd.concat([metric_multi, batch_metric], axis=1)\n",
    "        #batch_etho = load_data_from_keys(fpath, 'ethogram')\n",
    "    if metric == 'mean duration':\n",
    "        metric_multi = metric_multi/fps\n",
    "    metric_multi.index.name = 'cluster'\n",
    "    if '-1' or -1 in metric_multi.index:\n",
    "        try:\n",
    "            metric_multi = metric_multi.drop(-1)\n",
    "        except:\n",
    "            try:\n",
    "                metric_multi = metric_multi.drop('-1')\n",
    "            except:\n",
    "                pass\n",
    "    if metric == 'rel time in':\n",
    "        metric_multi = metric_multi/metric_multi.sum(axis=0)\n",
    "        #print(metric_multi.groupby(level=0, axis=1).mean())\n",
    "    #metric_multi.index = [eval(i) for i in metric_multi.index]\n",
    "        \n",
    "    ### stats\n",
    "    \n",
    "    all_ps_adju = pd.DataFrame(index=metric_multi.index)\n",
    "    all_ps_orig = pd.DataFrame(index=metric_multi.index)\n",
    "    all_conds = metric_multi.columns.get_level_values(0).unique()\n",
    "    N_tests = len(all_conds)-1#len(test_metrics) if not 'mean transitions' in test_metrics else len(test_metrics)+len(cluster_label)-1\n",
    "    print(N_tests)\n",
    "\n",
    "    stats_csv = pd.DataFrame([], columns = ['Experiment', 'Population', 'Condition', 'State', 'N', 'Mean Rank', 'Sum Rank', 'U1', 'U2', 'Mann-Whitney U', 'p', 'number of tests', 'bonferroni p'])\n",
    "    for i, cond in enumerate(all_conds):\n",
    "        U1s, ps = stats.mannwhitneyu(metric_multi[stat_pop], metric_multi[cond], axis=1, nan_policy='omit')\n",
    "        \n",
    "        ps_adjusted = ps*N_tests # bonferroni correction... like this correction done for each cluster and each condition seperatly across metrics tested\n",
    "        all_ps_adju[cond] = ps_adjusted\n",
    "        all_ps_orig[cond] = ps\n",
    "\n",
    "        # get number of conditions\n",
    "        cond_n = np.count_nonzero(~np.isnan(metric_multi[cond]), axis=1)\n",
    "        pop_n = np.count_nonzero(~np.isnan(metric_multi[stat_pop]), axis=1)\n",
    "        # calculuate U2 from U1\n",
    "        U2s = pop_n*cond_n - U1s\n",
    "        Us = np.min((U1s, U2s), axis=0)\n",
    "\n",
    "        # rank dataset, U1 or U2 belong to?, but strictly important, because U = min(U1,U2)\n",
    "        rank_multi = pd.concat([metric_multi[stat_pop], metric_multi[cond]], axis=1,\n",
    "                               keys={'pop':metric_multi[stat_pop].columns, 'cond':metric_multi[cond].columns,})\n",
    "        cond_idx, = np.where(rank_multi.columns.get_level_values(0)=='cond')\n",
    "        ranks = stats.rankdata(rank_multi, axis=1, nan_policy='omit')\n",
    "        cond_ranks = ranks[:,cond_idx]\n",
    "        cond_ranks_mean = np.nanmean(cond_ranks, axis=1)\n",
    "        cond_ranks_sum = np.nansum(cond_ranks, axis=1)\n",
    "\n",
    "        # create dict of current condition\n",
    "        stats_cond = {'Experiment': [fig_folder]*metric_multi.shape[0], \n",
    "                      'Population': [stat_pop]*metric_multi.shape[0], \n",
    "                      'Condition': [cond]*metric_multi.shape[0], \n",
    "                      'State': [cluster_group[eval(c)] for c in metric_multi.index], \n",
    "                      'N': cond_n, \n",
    "                      'Mean Rank': cond_ranks_mean, \n",
    "                      'Sum Rank': cond_ranks_sum, \n",
    "                      'U1': U1s, \n",
    "                      'U2': U2s, \n",
    "                      'Mann-Whitney U': Us, \n",
    "                      'p': ps, \n",
    "                      'number of tests': [N_tests]*metric_multi.shape[0], \n",
    "                      'bonferroni p': ps_adjusted}\n",
    "        # concat stats dict to dataframe stats_csv\n",
    "        stats_csv = pd.concat([stats_csv, pd.DataFrame(stats_cond)])\n",
    "\n",
    "    stats_csv.to_csv(outmetric+'.csv')\n",
    "\n",
    "    if use_bonferroni:\n",
    "        all_ps = all_ps_adju\n",
    "    else:\n",
    "        all_ps = all_ps_orig\n",
    "    ### make boxplots\n",
    "    if metric != 'mean transitions':\n",
    "        percentile = (np.round(np.nanpercentile(metric_multi,pctl_toplot[0])).astype(int),np.round(np.nanpercentile(metric_multi,pctl_toplot[1])).astype(int))\n",
    "\n",
    "        fig, ax = plt.subplots(figsize= (6*len(all_ps.columns),4))\n",
    "        N_across = []\n",
    "        for i, cond in enumerate(all_conds):\n",
    "            N_across.append(np.count_nonzero(~np.isnan(metric_multi[cond]), axis=1))\n",
    "            for j,c in enumerate(metric_multi.index):\n",
    "                c_ = eval(c)\n",
    "                c_ = c_[0] if isinstance(c_, tuple) else c_\n",
    "                p = all_ps.loc[c, cond]\n",
    "                hpos = j*len(all_conds)+i*.8\n",
    "                ax.boxplot(metric_multi[cond].loc[c][~np.isnan(metric_multi[cond].loc[c])], \n",
    "                            positions = [hpos],\n",
    "                            widths=.4,\n",
    "                            showfliers=showfliers,\n",
    "                            #showmeans =True, meanline = True,\n",
    "                            patch_artist = True, boxprops={'facecolor':cluster_color[c_]},medianprops={'color':'k'})\n",
    "        \n",
    "        for i, cond in enumerate(all_conds):\n",
    "            for j,c in enumerate(metric_multi.index):\n",
    "                p = all_ps.loc[c, cond]\n",
    "                hpos = j*len(all_conds)+i*.8\n",
    "                vpos = ax.get_ylim()[1]\n",
    "                if p < .05:\n",
    "                    #percentile[1]\n",
    "                    #vpos = vpos+vpos*.05*i #np.max(metric_multi.loc[str(c), cond])+\n",
    "                    ax.text(hpos, vpos,'*'*sig_stars(p), ha='left', va='bottom', rotation=45)\n",
    "                else:\n",
    "                    ax.text(hpos, vpos,'n.s.', ha='left', va='bottom', rotation=45)\n",
    "                ax.text(hpos, vpos*1.15,'N='+str(N_across[i][j]),ha='center', va='bottom')\n",
    "                    \n",
    "        #for c in metric_multi.index:\n",
    "        #    c_ = eval(c)\n",
    "        #    c_ = c_[0] if isinstance(c_, tuple) else c_\n",
    "        #    ax.text(c_*len(all_conds), -ax.get_ylim()[1]*.5, cluster_group[c_], ha='left', va='top', rotation=-45)\n",
    "                #plt.text(c+i*.2, texty, np.round(p,3))\n",
    "\n",
    "        common = (find_common(*[s.split('_') for s in include]))\n",
    "        uncommon = (find_uncommon(*[s.split('_') for s in include]))\n",
    "        #uncommon = [u for u in uncommon if not 'Exp' in u]\n",
    "\n",
    "        xlabels = np.repeat(uncommon,len(cluster_color)-1)\n",
    "        ax.set_xticklabels(xlabels, rotation=90)        \n",
    "        ax.set_ylabel(f'{metric}')\n",
    "        #prange5p = np.diff(percentile)*.05\n",
    "        #ax.set_ylim(percentile[0]-prange5p, percentile[1]+prange5p)\n",
    "        ax.spines[['right', 'top']].set_visible(False)\n",
    "        txt=json.dumps(stats_json['meta'],separators=('\\n',','))#+'\\nshown range in 0.5 to 99.5 percentile'\n",
    "        plt.title('_'.join(common),x=1.1,y=1.1)\n",
    "        plt.figtext(1, .9, txt, wrap=True, ha='left', va='center', rotation=-90)\n",
    "        #plt.savefig(outmetric+'.pdf', bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        metric_multi.index = pd.MultiIndex.from_tuples([eval(i) for i in metric_multi.index])\n",
    "        metric_multi = metric_multi.drop(-1, level = 0)\n",
    "        metric_multi = metric_multi.drop(-1, level = 1)\n",
    "        all_ps.index = pd.MultiIndex.from_tuples([eval(i) for i in all_ps.index])\n",
    "        ind_lvl = metric_multi.index.get_level_values(0).unique()\n",
    "        fig, axs = plt.subplots(len(ind_lvl), figsize= (13,12), sharex=True)\n",
    "        for i, c_from in enumerate(ind_lvl):\n",
    "            for j,cond in enumerate(all_conds):\n",
    "                for k, c_to in enumerate(ind_lvl):\n",
    "                    if i == k:\n",
    "                        continue\n",
    "                    p = all_ps[cond].xs((c_from, c_to))\n",
    "                    hpos = k*len(all_conds)+j*.8\n",
    "                    axs[i].boxplot(metric_multi[cond].xs((c_from, c_to))[~np.isnan(metric_multi[cond].xs((c_from, c_to)))], \n",
    "                                positions = [hpos],\n",
    "                                widths=.4,\n",
    "                                showfliers=showfliers,\n",
    "                                patch_artist = True, boxprops={'facecolor':cluster_color[c_to]},medianprops={'color':'k'})\n",
    "                    if p <= .05:\n",
    "                        axs[i].text(hpos, .9,'*'*sig_stars(p), ha='left', va='bottom', rotation=45)\n",
    "                    else:\n",
    "                        axs[i].text(hpos, .9,'n.s.', ha='left', va='bottom', rotation=45)\n",
    "            axs[i].set_ylim(0,1)\n",
    "            axs[i].set_ylabel(f'{cluster_group[c_from]}')\n",
    "            axs[i].spines[['right', 'top']].set_visible(False)\n",
    "            #axs[-1].text(i*len(all_conds)+len(all_conds)/3, -.9, cluster_group[c_from], ha='center', va='center', rotation=-0)\n",
    "            \n",
    "        #axs[0].set_title(metric)\n",
    "        axs[-1].set_xticklabels(np.tile(np.repeat([s.split('_')[1] for s in include],len(cluster_color)-2),len(cluster_color)-1), rotation=-90)#-2 in first repeat because without none and self\n",
    "        #plt.savefig(outmetric+'.pdf', bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6*len(all_ps.columns),4))\n",
    "\n",
    "        for i, c_ in enumerate(ind_lvl):\n",
    "            for j,cond in enumerate(all_conds):\n",
    "                p = all_ps[cond].xs((c_, c_))\n",
    "                hpos = i*len(all_conds)+j*.8\n",
    "                d = metric_multi[cond].xs((c_, c_))[~np.isnan(metric_multi[cond].xs((c_, c_)))]\n",
    "                ax.boxplot(d, \n",
    "                            positions = [hpos],\n",
    "                            widths=.4,\n",
    "                            showfliers=showfliers,\n",
    "                            patch_artist = True, boxprops={'facecolor':cluster_color[c_]},medianprops={'color':'k'})\n",
    "                if p < .05:\n",
    "                    ax.text(hpos, 1,'*'*sig_stars(p), ha='left', va='bottom', rotation= 45)\n",
    "                else:\n",
    "                    ax.text(hpos, 1,'n.s.', ha='left', va='bottom', rotation=45)\n",
    "                ax.text(hpos, 1.15,'N='+str(len(d)),ha='center', va='bottom')\n",
    "                        \n",
    "            ax.set_ylim(0,1)\n",
    "            ax.set_ylabel(f'mean persistance (self-loop)')\n",
    "            ax.spines[['right', 'top']].set_visible(False)\n",
    "            #ax.text(i*len(all_conds)+len(all_conds)/3, -ax.get_ylim()[1]*.5, cluster_group[c_], ha='center', va='top', rotation=-45)\n",
    "        \n",
    "        ax.set_xticklabels(np.tile([s.split('_')[1] for s in include],len(cluster_color)-1), rotation=-0)\n",
    "        #plt.savefig(outmetric+'_self.pdf', bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e4a129-3fc1-46dc-83d6-52f94316285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_substring(lst, splitter='_'):\n",
    "    compare_to = lst[0].split(splitter)\n",
    "    common, uncommon = [], []\n",
    "    for string in lst[1:]:\n",
    "        subs = string.split(splitter)\n",
    "        for s in subs:\n",
    "            if s in compare_to:\n",
    "                common.append(s)\n",
    "            else:\n",
    "                uncommon.append(s)\n",
    "    uncommon += compare_to.remove(common)\n",
    "    return common, uncommon\n",
    "\n",
    "# Test the function\n",
    "print(common_substring(include))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d846d787-d871-41a7-b902-21ba9232cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst1 = ['this', 'that', 'these']\n",
    "lst2 = ['this', 'that', 'those']\n",
    "lst3 = ['this', 'that', 'them']\n",
    "common = (find_common(*[s.split('_') for s in include]))  # Output: [5]\n",
    "uncommon = (find_uncommon(*[s.split('_') for s in include]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44eef8-b997-4d9e-9384-f6195b3b0bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in ['mean transitions']:\n",
    "    outmetric = os.path.join(outpath, f\"{datestr}_{''.join(metric.split(' '))}_Utest\")\n",
    "    \n",
    "    metric_multi = None\n",
    "    for fn, fpath in loc_all.items():\n",
    "        batch_metric = pd.DataFrame(load_data_from_keys(fpath, metric)).droplevel(0, axis=1)\n",
    "        batch_metric.columns = pd.MultiIndex.from_product([[fn], batch_metric.columns])\n",
    "        if metric_multi is None:\n",
    "            metric_multi = batch_metric\n",
    "        else:\n",
    "            metric_multi = pd.concat([metric_multi, batch_metric], axis=1)\n",
    "        #batch_etho = load_data_from_keys(fpath, 'ethogram')\n",
    "    if metric == 'mean duration':\n",
    "        metric_multi = metric_multi/fps\n",
    "    metric_multi.index.name = 'cluster'\n",
    "\n",
    "    all_conds = metric_multi.columns.get_level_values(0).unique()\n",
    "    id_cond = [c.split('_')[0] for c in all_conds]\n",
    "    metric_multi.index = pd.MultiIndex.from_tuples([eval(i) for i in metric_multi.index])\n",
    "    metric_multi = metric_multi.drop(-1, level = 0)\n",
    "    metric_multi = metric_multi.drop(-1, level = 1)\n",
    "    for i, cond in enumerate(all_conds):\n",
    "        pass\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a78ca2-0695-490d-b4f1-d82aeebeac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_transition = metric_multi['bac_data_batch.json'].mean(axis=1).unstack(level=0)\n",
    "fr_transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acfbc5b-6fa9-4907-8077-aca2077c0c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_larvae = metric_multi['larvae_data_batch.json'].mean(axis=1).unstack(level=0)\n",
    "tr_larvae_norm = (tr_larvae/tr_larvae.sum(axis=0)).fillna(0)\n",
    "plt.imshow(tr_larvae_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e4f667-46d5-49ec-beff-2479561db1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "testbatch = 'tbh1_larvae_batch.json'\n",
    "fr_transition = metric_multi[testbatch].mean(axis=1).unstack(level=0)\n",
    "fr_transition_norm = (fr_transition/fr_transition.sum(axis=0)).fillna(0)\n",
    "plt.imshow(fr_transition_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29016ed6-1f76-4ef9-8afc-38e4d94ea824",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_rel = fr_transition_norm-tr_larvae_norm\n",
    "plt.imshow(trans_rel, cmap='seismic',vmin=-.5,vmax=.5)\n",
    "plt.title(f'{testbatch} against larvae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80eafd-137c-4126-8ce6-cd78d30ce72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_json = {'meta':{'metric': metric,\n",
    "                          'statistical test': 'Mann-Whitney-U-Test, two-sided',\n",
    "                          'bonferroni correction': 'statsmodels...multipletests',\n",
    "                          'population': stat_pop},\n",
    "              'adjusted ps': all_ps.to_dict(),\n",
    "              'Mann-Whitney U': all_U1s.to_dict(),\n",
    "              'unadjusted ps': all_ps_orig.to_dict(),\n",
    "              }\n",
    "jsnF = json.dumps(stats_json, indent = 4)\n",
    "with open(os.path.join(outpath, f'{datestr}_{metric}_Utest.json'), \"w\") as outfile:\n",
    "    outfile.write(jsnF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5578a9-b25f-4d11-8012-710a71d10bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "li = [eval(i) for i in metric_multi.index]\n",
    "type(li[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55cf402-4375-491a-bec0-160566a660fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "op.itemgetter('metric', 'statistical test', 'bonferroni correction', 'population')(stats_json)\n",
    "op.getitem(stats_json,'metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce2244c-ad50-4d43-8adf-d687dd94de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "U1s, pvs = stats.mannwhitneyu(metric_multi[stat_pop], metric_multi['bac_data_batch.json'], axis=1, nan_policy='omit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d4a4d4-cb23-4e15-9113-820f7e1051f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "___,ps_adjusted,__,_ = multipletests(pvs, method='bonferroni')\n",
    "np.round(ps_adjusted,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23acf27-94f7-4dfb-bb7d-e569a797ff33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    ### Load data ################################################\n",
    "    #data_batch = load_tolist(fpath, droplabelcol=False)\n",
    "    #data_batch_concat = pd.concat([d for d in data_batch], axis=0)\n",
    "    \n",
    "    #y_batch_concat = data_batch_concat['prediction']\n",
    "    #y_batch = [d['prediction'] for d in data_batch]\n",
    "\n",
    "    \n",
    "    ### Batch summary ############################################\n",
    "    for i,d in enumerate(loc_summ):\n",
    "        summ_ = pd.read_csv(d)\n",
    "        dev_ = (~summ_.isna()).astype(int)\n",
    "        summ_ = summ_.fillna(0)\n",
    "        #trans_col_ = [[c for c in fr_transition_],[c for c in fr_transition_]]\n",
    "        \n",
    "        if i == 0:\n",
    "            summ = summ_\n",
    "            devider = dev_\n",
    "        else:\n",
    "            summ += summ_\n",
    "            devider += dev_\n",
    "    summary = summ/devider\n",
    "    #summary.to_csv(os.path.join(outpath, f'{data_str}_batch_summary.csv'))\n",
    "    \n",
    "    total_dur = summary.duration_mean*summary.duration_count\n",
    "    total_dur_rel = total_dur/total_dur.sum()\n",
    "    \n",
    "    ### Batch transitions ############################################\n",
    "    #batch_transitions = np.full((len(loc_trans),9,9), np.nan) # with shape recordings,cluster from, cluster to        \n",
    "    for i,d in enumerate(loc_trans):\n",
    "        fr_transition_ = pd.read_csv(d, index_col=0)\n",
    "        #batch_transitions[i] = fr_transition_\n",
    "        trans_col_ = [[c for c in fr_transition_],[c for c in fr_transition_]]\n",
    "        if i == 0:\n",
    "            fr_transition = fr_transition_\n",
    "            trans_col = trans_col_\n",
    "        elif trans_col_ == trans_col:\n",
    "            #print('this')\n",
    "            fr_transition += fr_transition_\n",
    "        else:\n",
    "            print('WARNING')\n",
    "\n",
    "    # merge all transitions across animals and experiments\n",
    "    #if all_transitions is None:\n",
    "    #    all_transitions = np.array([batch_transitions])\n",
    "    #else:\n",
    "    #    all_transitions,batch_transitions = padtoequalsize(all_transitions, batch_transitions, [-3])\n",
    "    #     all_transitions = np.concatenate([all_transitions,[batch_transitions]],axis=0)\n",
    "\n",
    "    # calculate the normalised transition and cluster\n",
    "    # cluster behaviors with single linkage\n",
    "    Z = linkage(fr_transition, 'single', optimal_ordering=True)\n",
    "    # extract closest cluster\n",
    "    ordering = np.concatenate((Z[::-1,0],Z[:,1]))\n",
    "    ordering = ordering[ordering<len(cluster_label)]-1\n",
    "    # calculate normalised transitions\n",
    "    fr_trans_norm = (fr_transition/fr_transition.sum(axis=0)).fillna(0)\n",
    "    fr_transition_clust = fr_trans_norm.iloc[ordering,ordering]\n",
    "    fr_trans_norm.to_csv(os.path.join(outpath, f'{data_str}_batch_transitions_2.csv'), index=False)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ### Batch durations ############################################\n",
    "    #batch_durations = None\n",
    "    for rec in loc_onoff:\n",
    "        with open(rec,'r') as jf:\n",
    "            onoff = json.load(jf)\n",
    "        #duration = pd.DataFrame(dtype='float64')\n",
    "        for cluster in onoff: #for bout in cluster\n",
    "            cluster_dur = []\n",
    "            for bout in onoff[cluster]: #for bout in cluster\n",
    "                cluster_dur.append(bout[1])\n",
    "            #duration = pd.concat([duration,pd.DataFrame([cluster_dur],index=[eval(cluster)],dtype='float64')],axis=0)\n",
    "            \n",
    "        cl_present = duration.index\n",
    "        cl_range = range(min(cl_present), max(cl_present)+1)\n",
    "        duration = duration.reindex(cl_range)\n",
    "        \n",
    "        if batch_durations is None:\n",
    "            batch_durations = np.array([duration.values])\n",
    "        else:\n",
    "            batch_durations, duration = padtoequalsize(batch_durations, duration, [-1,-2])\n",
    "            batch_durations = np.concatenate([batch_durations,[duration]],axis=0)\n",
    "    \n",
    "    if all_durations is None:\n",
    "        all_durations = np.array([batch_durations])\n",
    "    else:\n",
    "        all_durations,batch_durations = padtoequalsize(all_durations, batch_durations, [-1,-2,-3])\n",
    "        all_durations = np.concatenate([all_durations,[batch_durations]],axis=0)\n",
    "    \n",
    "        \n",
    "    #############\n",
    "    ### plots ###\n",
    "    #############\n",
    "    \n",
    "    if plot:\n",
    "        \"\"\"\n",
    "        transition_plot = vis.transition_plotter(fr_trans_norm.values, cluster_color, node_alpha=summary['duration_relative'].fillna(0).tolist())\n",
    "        plt.text(1.5, -1, f'{data_str}\\nN = {len(loc_trans)}', fontsize=12)\n",
    "        plt.title(f\"transitions of {data_str}\")\n",
    "        plt.savefig(os.path.join(outpath, f'{data_str}_batch_transitions.pdf'))\n",
    "        plt.show()\"\"\"\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        axs1 = fig.add_axes([0, .895, .2, .805])\n",
    "        axs1.axis('off')\n",
    "        dn = dendrogram(Z, orientation='left',ax= axs1)\n",
    "        axs2 = fig.add_axes([0.33, .9, .8, .8])\n",
    "        im = axs2.imshow(fr_transition_clust,norm=colors.PowerNorm(.4,vmax=1))#vmin=0,vmax=.2)#\n",
    "        axs2.set_xticks(range(len(cluster_group)))\n",
    "        axs2.set_xticklabels([cluster_group[k] for k in ordering], rotation=45,ha=\"center\")\n",
    "        axs2.set_yticks(range(len(cluster_group)))\n",
    "        axs2.set_yticklabels([cluster_group[k] for k in ordering])\n",
    "        cbar = axs2.figure.colorbar(im, ax=axs2)\n",
    "        cbar.ax.set_ylabel(\"X^0.4 normalization\", rotation=90, labelpad= 6)\n",
    "        axs2.set_title(f\"transitions of {data_str} per sec (x to y)\")\n",
    "        plt.savefig(os.path.join(outpath, f'{data_str}_batch_transitheatmap_clust.pdf'),bbox_inches = \"tight\")\n",
    "        plt.show()\n",
    "    \n",
    "        if WT_ordering:\n",
    "            fr_transition_WT = fr_trans_norm.iloc[WT_ordering,WT_ordering]\n",
    "            fig = plt.figure()\n",
    "            axs1 = fig.add_axes([0, .895, .2, .805])\n",
    "            axs1.axis('off')\n",
    "            #dn = dendrogram(Z, orientation='left',ax= axs1)\n",
    "            axs2 = fig.add_axes([0.33, .9, .8, .8])\n",
    "            im = axs2.imshow(fr_transition_WT,norm=colors.PowerNorm(.4,vmax=1))#vmin=0,vmax=.2)#\n",
    "            axs2.set_xticks(range(len(cluster_group)))\n",
    "            axs2.set_xticklabels([cluster_group[k] for k in WT_ordering], rotation=45,ha=\"center\")\n",
    "            axs2.set_yticks(range(len(cluster_group)))\n",
    "            axs2.set_yticklabels([cluster_group[k] for k in WT_ordering])\n",
    "            cbar = axs2.figure.colorbar(im, ax=axs2)\n",
    "            cbar.ax.set_ylabel(\"X^0.4 normalization\", rotation=90, labelpad= 6)\n",
    "            axs2.set_title(f\"transitions of {data_str} per sec (x to y)\")\n",
    "            plt.savefig(os.path.join(outpath, f'{data_str}_batch_transitheatmap_WTclust.pdf'),bbox_inches = \"tight\")\n",
    "            plt.show()\n",
    "        \n",
    "    \n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(2,5))\n",
    "        b = ax.barh(range(len(total_dur_rel)),total_dur_rel, color=list(cluster_color.values())[1:])\n",
    "        ax.set_yticks(range(len(cluster_group)-1))\n",
    "        ax.set_yticklabels([cluster_group[k] for k in cluster_group][1:])\n",
    "        ax.bar_label(b, label_type='edge', fmt='%.2g', padding=3)\n",
    "        ax.invert_yaxis()\n",
    "        plt.xlabel(f\"total rel. duration\")\n",
    "        plt.title(data_str)\n",
    "        plt.xlim(0,.6)\n",
    "        plt.savefig(os.path.join(outpath, f'{data_str}_batch_totaldur.pdf'),bbox_inches = \"tight\")\n",
    "        plt.show()\n",
    "    \n",
    "        # Calculate onoff sets\n",
    "        onoff = proc.onoff_dict(y_batch, labels =np.unique(y_batch_concat))\n",
    "    \n",
    "        fig = plt.figure(figsize=(.6*len(onoff.keys()),3), layout='constrained')\n",
    "        for k in onoff.keys():\n",
    "            if not np.isnan(k):\n",
    "                durs = pd.DataFrame(onoff[k])[1]/30\n",
    "                plt.boxplot(durs, positions=[k], widths=.5,sym='', patch_artist = True, boxprops={'facecolor':cluster_color[k]},medianprops={'color':'k'})\n",
    "        plt.xticks(range(len(cluster_group)-1),[cluster_group[k] for k in cluster_group][1:], rotation=45)\n",
    "        plt.ylabel(\"sec\")\n",
    "        plt.ylim(0,30)\n",
    "        plt.title(f\"duration of {data_str} (without fliers)\")\n",
    "        plt.savefig(os.path.join(outpath, f'{data_str}_batch_durboxplot.pdf'),bbox_inches = \"tight\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca73a0d-443b-4b74-9143-f313252531da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_durations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3e1537-aaa3-48b6-9e56-7fb7042dd581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6958e3-3b1c-4df6-8df0-a1fd990ddd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce1cabf-c47a-401a-b8a3-285c40b42e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_withnans_alignright(df_):\n",
    "    df = df_.copy()\n",
    "    for i in range(len(df.columns)):\n",
    "        nulls = df.iloc[np.where(df.iloc[:,i].notnull())[0][-1]+1:,i]\n",
    "        notnulls = df.iloc[:np.where(df.iloc[:,i].notnull())[0][-1],i]\n",
    "        right_aligned = pd.concat([nulls,notnulls], axis=0).reset_index(drop=True)\n",
    "        df.iloc[:,i] = right_aligned\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e006eda-6be7-4aa9-81fa-5518d789c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(96)\n",
    "bout_all = pd.DataFrame([])\n",
    "bout_pre_all = pd.DataFrame([])\n",
    "y_pre_all = pd.DataFrame([])\n",
    "bout_post_all = pd.DataFrame([])\n",
    "y_post_all = pd.DataFrame([])\n",
    "feat = 'velocity'\n",
    "cl = 1\n",
    "rdm_subset = np.random.choice(len(onoff[cl]), 100, replace=False)\n",
    "rdm_ons =[]\n",
    "for i,oo in enumerate(rdm_subset):\n",
    "    rdm_ons.append(onoff[cl][oo][0])\n",
    "    onset = onoff[cl][oo][0]\n",
    "    offset = onoff[cl][oo][0]+onoff[cl][oo][1]\n",
    "    bout = data_batch_concat.reset_index(drop=True)[onset:offset].reset_index(drop=True)\n",
    "    if all(bout[feat].isnull()):\n",
    "        print(i)\n",
    "        continue\n",
    "    bout_pre = data_batch_concat.reset_index(drop=True)[onset-60:onset].reset_index(drop=True)\n",
    "    bout_post = data_batch_concat.reset_index(drop=True)[offset:offset+60].reset_index(drop=True)\n",
    "    \n",
    "    bout_all = pd.concat([bout_all,bout[feat]], axis=1)\n",
    "\n",
    "    bout_pre_all = pd.concat([bout_pre_all,bout_pre[feat]], axis=1)\n",
    "    y_pre_all = pd.concat([y_pre_all,bout_pre['prediction']], axis=1)\n",
    "    bout_post_all = pd.concat([bout_post_all,bout_post[feat]], axis=1)\n",
    "    y_post_all = pd.concat([y_post_all,bout_post['prediction']], axis=1)\n",
    "\n",
    "bout_all.columns = list(range(len(bout_all.columns)))\n",
    "bout_pre_all.columns = list(range(len(bout_pre_all.columns)))\n",
    "y_pre_all.columns = list(range(len(y_pre_all.columns)))\n",
    "y_pre_color = y_pre_all[:150].replace(cluster_color)\n",
    "bout_post_all.columns = list(range(len(bout_post_all.columns)))\n",
    "y_post_all.columns = list(range(len(y_post_all.columns)))\n",
    "y_post_color = y_post_all[:150].replace(cluster_color)\n",
    "\n",
    "bout_all_right = df_withnans_alignright(bout_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e4158-0d21-4595-ac85-ec09628a9921",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "mask = data_batch_concat.prediction.isin([7])\n",
    "#fig = px.lines(bout_all, color = y_bout_all, width=800, height=400)\n",
    "fig = make_subplots(rows=1, cols=2)\n",
    "for trace in bout_all:\n",
    "    bout_ = bout_all[trace].iloc[:90].dropna().reset_index(drop=True)\n",
    "    bout_r_ = bout_all_right[trace].iloc[-90:].dropna().reset_index(drop=True)\n",
    "    pre_ = bout_pre_all[trace][y_pre_all[trace]==y_pre_all[trace].iloc[-1]].reset_index(drop=True)\n",
    "    post_ = bout_post_all[trace][y_post_all[trace]==y_post_all[trace].iloc[-1]].reset_index(drop=True)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=np.arange(len(bout_)), y=bout_, line=dict(color=cluster_color[cl]),mode='lines'),\n",
    "                 row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=np.arange(-len(pre_),0), y=pre_, line=dict(color=y_pre_color[trace].iloc[-1]),mode='lines'),\n",
    "                 row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=np.arange(-len(bout_r_),0), y=bout_r_, line=dict(color=cluster_color[cl]),mode='lines'),\n",
    "                 row=1, col=2)\n",
    "    fig.add_trace(go.Scatter(x=np.arange(len(post_)), y=post_, line=dict(color=y_post_color[trace].iloc[-1]),mode='lines'),\n",
    "                 row=1, col=2)\n",
    "\n",
    "    \n",
    "mean_ = pd.concat([bout_pre_all,bout_all],axis=0).reset_index(drop=True).mean(axis=1).iloc[:150]\n",
    "mean_r_ = pd.concat([bout_all_right, bout_post_all],axis=0).reset_index(drop=True).mean(axis=1).iloc[-150:]\n",
    "fig.add_trace(go.Scatter(x=np.arange(-60,90), y=mean_, line=dict(color='black',width=3),mode='lines'),\n",
    "                 row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=np.arange(-90,60), y=mean_r_, line=dict(color='black',width=3),mode='lines'),\n",
    "                 row=1, col=2)\n",
    "\n",
    "fig.add_vline(x=0, line=dict(color=\"Grey\",width=5))\n",
    "fig.update_xaxes(dtick=15, title=dict(text='Time [frames]'))\n",
    "fig.update_yaxes(range=[0, max(bout_all.max().max(),bout_pre_all.max().max(),bout_post_all.max().max())],title=dict(text=f'{feat}'))\n",
    "fig.update_layout(\n",
    "    width=1200,\n",
    "    height=500,\n",
    "    xaxis = dict(\n",
    "        tickmode = 'linear',\n",
    "        dtick = 15),\n",
    "    showlegend=False,\n",
    "    template='plotly_white',\n",
    "    title=dict(text=f\"Traces of {cluster_group[cl]} aligned at prediction onset (l) and offset (r)\", font=dict(size=16), x =.5, xanchor='center'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dbbba1-5f2c-4745-9917-f327189b7283",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([bout_pre_all,bout_all], axis=0).reset_index(drop=True)[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ddd09a-4b9b-4827-91ab-9e80261e0ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "onoff_ = pd.json_normalize(onoff).to_dict(orient='split')\n",
    "onoff_rev = {}\n",
    "for l,c in zip(onoff_['data'][0],onoff_['columns']):\n",
    "    for oo in l:\n",
    "        onoff_rev[oo[0]] = (c, oo[1])\n",
    "\n",
    "onoff_rev_df = pd.DataFrame(onoff_rev).T\n",
    "onoff_rev_df = onoff_rev_df.reindex(sorted(onoff_rev_df.index), axis=0)\n",
    "\n",
    "onoff_rev_dfidx = onoff_rev_df.index.to_series().reset_index(drop=True)\n",
    "prebout_idx = (onoff_rev_dfidx[onoff_rev_dfidx.isin(rdm_ons)].index - 1).tolist()\n",
    "\n",
    "prebout_c, prebout_dur, prebout = onoff_rev_df.iloc[prebout_idx,0],onoff_rev_df.iloc[prebout_idx,1], onoff_rev_df.index[prebout_idx].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772f242b-35e0-4594-af30-ddcf25a7e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(8,5), sharey=True)\n",
    "axs[0].plot(range(0,90),bout_all[:90], c=cluster_color[cl],alpha=.7)\n",
    "for b,d,c in zip(prebout,prebout_dur,prebout_c):\n",
    "    axs[0].plot(range(-d if d < 60 else -60,0),data_batch_concat.reset_index(drop=True).iloc[b:b+d][feat][-d if d < 60 else -60:], c=cluster_color[c],alpha=.7)\n",
    "axs[0].plot(range(0,90),bout_all[:90].median(axis=1), c='#252422', linewidth=2)\n",
    "axs[0].axvline(0)\n",
    "axs[0].set_title(\"aligned beginning\")\n",
    "axs[1].plot(range(-90,0),bout_all_right[-90:], c=cluster_color[cl],alpha=.7)\n",
    "axs[1].plot(range(-90,0),bout_all_right[-90:].median(axis=1), c='#252422', linewidth=2)\n",
    "axs[1].axvline(0)\n",
    "axs[1].set_title(\"aligned end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfee0dc-6dfe-41d5-aaf2-dae30852b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre_all[trace][y_pre_all[trace]==y_pre_all[trace].iloc[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d97d24e-6c8e-4b5f-814a-7e5479d9dc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "Z = linkage(fr_transition, 'single', optimal_ordering=True)\n",
    "ordering = np.concatenate((Z[::-1,0],Z[:,1]))\n",
    "ordering = ordering[ordering<=8]\n",
    "fr_trans_norm = (fr_transition/fr_transition.sum(axis=0)).fillna(0)\n",
    "fr_transition_clust = fr_transition.iloc[ordering,ordering]\n",
    "fr_trans_clust_norm = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea81f16-6c0b-42b1-b444-06f98925dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axs1 = fig.add_axes([0, .895, .2, .805])\n",
    "axs1.axis('off')\n",
    "dn = dendrogram(Z, orientation='left',ax= axs1)\n",
    "axs2 = fig.add_axes([0.33, .9, .8, .8])\n",
    "im = axs2.imshow(fr_transition_clust,norm=colors.PowerNorm(.4,vmax=1))#vmin=0,vmax=.2)#\n",
    "axs2.set_xticks(range(len(cluster_group)-1))\n",
    "axs2.set_xticklabels([cluster_group[k] for k in ordering], rotation=45,ha=\"center\")\n",
    "axs2.set_yticks(range(len(cluster_group)-1))\n",
    "axs2.set_yticklabels([cluster_group[k] for k in ordering])\n",
    "cbar = axs2.figure.colorbar(im, ax=axs2)\n",
    "cbar.ax.set_ylabel(\"X^0.4 normalization\", rotation=90, labelpad= 6)\n",
    "axs2.set_title(f\"transitions of {data_str} per sec (x to y)\")\n",
    "plt.savefig(os.path.join(out_predicted, os.path.basename(outpath)+'_batch_transitheatmap_clust.pdf'),bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1805f81-4486-477f-bd8c-2ed01661e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_trans_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0aeb5e-7094-4b85-9e08-bb9d92af84c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(2/.6*1,5))\n",
    "b = ax.barh(range(len(total_dur_rel)),total_dur_rel, color=list(cluster_color.values())[1:])\n",
    "ax.set_yticks(range(len(cluster_group)-1))\n",
    "ax.set_yticklabels([cluster_group[k] for k in cluster_group][1:])\n",
    "ax.bar_label(b, label_type='edge', fmt='%.2g', padding=3)\n",
    "ax.invert_yaxis()\n",
    "plt.xlabel(f\"total rel. duration\")\n",
    "plt.title(data_str)\n",
    "plt.xlim(0,1)\n",
    "plt.savefig(os.path.join(out_predicted, os.path.basename(outpath)+'_batch_totaldur.pdf'),bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a89a61-fad6-40ec-b19e-995d7aedf8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    data_describe = data_batch_concat.groupby(y_batch_concat).describe().T.loc[idx[:, ['mean','std','count']], :].sort_index(level=0).T\n",
    "    dur_describe = pd.DataFrame(dur, columns=['duration']).groupby(transi).describe().T.loc[idx[:, ['mean','std','count']], :].sort_index(level=0).T\n",
    "    dur_describe['duration','relative'] = pd.DataFrame(dur, columns=['duration']).groupby(transi).apply(lambda cd: cd.sum()/len(y_batch_concat))\n",
    "    summary = pd.concat([dur_describe, data_describe], axis=1)\n",
    "    summary.index.name = 'cluster'\n",
    "    summary = summary.T.reset_index(drop=True).set_index(summary.T.index.map('_'.join)).T\n",
    "    summary = summary.set_index(summary.index.astype(int))\n",
    "    summary = summary.reindex([k for k in cluster_label if k != -1])\n",
    "    summary.to_csv(os.path.join(out_predicted, os.path.basename(outpath)+'_batch_summary.csv'))\n",
    "\n",
    "    #### Older Version\n",
    "    \n",
    "    for i,d in enumerate(data_batch):\n",
    "        frame = d['prediction'].rolling(30).apply(lambda s: s.mode()[0])[29::30].values.flatten()\n",
    "        trans_col_,fr_transition_ = crosstab(frame[1:], frame[:-1], levels=([k for k in cluster_label if k != -1],[k for k in cluster_label if k != -1]))\n",
    "        #fr_transition_ = pd.read_csv() ##################################read transitions.csv should look like normal fr_transition, get trans_col from header\n",
    "        if i == 0:\n",
    "            fr_transition = fr_transition_\n",
    "            trans_col = trans_col_\n",
    "        if trans_col_ == trans_col:\n",
    "            fr_transition += fr_transition_\n",
    "        else:\n",
    "            print('WARNING')\n",
    "        #fr_transition/fr_transition.sum(axis=0)\n",
    "    \n",
    "    #othersum_axis0 = fr_transition.sum(axis=0)-fr_transition.diagonal()\n",
    "    #transition_toother = fr_transition/othersum_axis0\n",
    "    #transition_self = fr_transition.diagonal()/(othersum_axis0+fr_transition.diagonal())\n",
    "    #np.fill_diagonal(transition_toother, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env2",
   "language": "python",
   "name": "sklearn-env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
